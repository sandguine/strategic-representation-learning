{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0749f96",
   "metadata": {},
   "source": [
    "# Strategic Representation Learning: Toy Experiments\n",
    "\n",
    "This notebook demonstrates a minimal experiment for evaluating soft strategic equivalence classes across three types of multi-agent interactions:\n",
    "\n",
    "- **Competitive** (e.g., Matching Pennies)\n",
    "- **Mixed-Motive** (e.g., asymmetric Coin Game)\n",
    "- **Cooperative** (e.g., Overcooked-style coordination)\n",
    "\n",
    "We show that policies with similar effects on the ego agent's soft best response (BR) form soft strategic equivalence classes (SECs), and that SEC size and entropy vary meaningfully across settings.\n",
    "\n",
    "## Mathematical Framework\n",
    "\n",
    "### 1. Soft Best Response\n",
    "The soft best response of agent $i$ to co-policy $\\pi_{-i}$ is defined as:\n",
    "\n",
    "$$BR^\\tau_i(a_i \\mid \\pi_{-i}) = \\frac{\\exp(Q_{\\pi_{-i}}(a_i)/\\tau)}{\\sum_{a'} \\exp(Q_{\\pi_{-i}}(a')/\\tau)}$$\n",
    "\n",
    "where $\\tau$ is the temperature parameter controlling the softness of the response.\n",
    "\n",
    "### 2. Strategic InfoNCE Loss\n",
    "The contrastive loss for learning strategic embeddings:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{InfoNCE}} = -\\log \\frac{f(a^+, h(\\pi_{-i}))}{\\sum_j f(a^-_j, h(\\pi_{-i}))}$$\n",
    "\n",
    "where $f$ is the similarity function between actions and embeddings.\n",
    "\n",
    "### 3. ΔSEC(t) Metric\n",
    "The reduction in strategic ambiguity over time:\n",
    "\n",
    "$$\\Delta \\text{SEC}(t) = H(S_{t-1}) - H(S_t)$$\n",
    "\n",
    "where $H(S_t)$ is the entropy of the strategic neighborhood at time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be8bf08a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Math\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "def softmax(q_values, tau=1.0):\n",
    "    \"\"\"Compute softmax with temperature parameter tau.\n",
    "    \n",
    "    Args:\n",
    "        q_values: Array of Q-values\n",
    "        tau: Temperature parameter (lower = harder response)\n",
    "    \n",
    "    Returns:\n",
    "        Softmax probabilities\n",
    "    \"\"\"\n",
    "    q = np.array(q_values)\n",
    "    q = q - np.max(q)  # numerical stability\n",
    "    return np.exp(q / tau) / np.sum(np.exp(q / tau))\n",
    "\n",
    "class StrategicEmbedding(nn.Module):\n",
    "    \"\"\"Neural network for learning strategic embeddings.\"\"\"\n",
    "    embedding_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(64)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.embedding_dim)(x)\n",
    "        return x\n",
    "\n",
    "def create_train_state(rng, input_dim, embedding_dim, learning_rate):\n",
    "    \"\"\"Creates initial training state.\"\"\"\n",
    "    model = StrategicEmbedding(embedding_dim=embedding_dim)\n",
    "    params = model.init(rng, jnp.ones((1, input_dim)))['params']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        tx=tx\n",
    "    )\n",
    "\n",
    "@jit\n",
    "def compute_infonce_loss(anchor_br, positive_br, negative_brs, tau=1.0):\n",
    "    \"\"\"Compute InfoNCE loss between best responses.\n",
    "    \n",
    "    Args:\n",
    "        anchor_br: Soft best response for anchor policy\n",
    "        positive_br: Soft best response for positive policy\n",
    "        negative_brs: List of soft best responses for negative policies\n",
    "        tau: Temperature parameter\n",
    "    \n",
    "    Returns:\n",
    "        InfoNCE loss value\n",
    "    \"\"\"\n",
    "    # Compute similarities using KL divergence\n",
    "    pos_sim = -entropy(anchor_br, positive_br)\n",
    "    neg_sims = jnp.array([-entropy(anchor_br, neg_br) for neg_br in negative_brs])\n",
    "    \n",
    "    # Compute InfoNCE loss\n",
    "    logits = jnp.concatenate([jnp.array([pos_sim]), neg_sims]) / tau\n",
    "    exp_logits = jnp.exp(logits - jnp.max(logits))\n",
    "    loss = -logits[0] + jnp.log(jnp.sum(exp_logits))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "@jit\n",
    "def train_step(state, batch):\n",
    "    \"\"\"Perform a single training step.\"\"\"\n",
    "    def loss_fn(params):\n",
    "        anchor_br, positive_br, negative_brs = batch\n",
    "        loss = compute_infonce_loss(anchor_br, positive_br, negative_brs)\n",
    "        return loss, None\n",
    "\n",
    "    grad_fn = grad(loss_fn, has_aux=True)\n",
    "    grads, _ = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a122b7",
   "metadata": {},
   "source": [
    "### Define Policy Q-values and Strategic Influence\n",
    "\n",
    "Each co-policy defines a vector of Q-values from the ego agent's perspective. These determine the soft best response distributions. We also define the strategic influence as the change in the ego agent's best response distribution.\n",
    "\n",
    "The strategic influence between policies $\\pi_i$ and $\\pi_j$ is measured as:\n",
    "\n",
    "$$I(\\pi_i, \\pi_j) = D_{KL}(BR^\\tau(\\pi_i) \\parallel BR^\\tau(\\pi_j))$$\n",
    "\n",
    "where $D_{KL}$ is the Kullback-Leibler divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eab66372",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define Q-values for each setting\n",
    "competitive_qs = {\n",
    "    \"always_head\": [1, -1],\n",
    "    \"always_tail\": [-1, 1],\n",
    "    \"random\": [0, 0],\n",
    "}\n",
    "\n",
    "mixed_qs = {\n",
    "    \"greedy_red\": [0.5, 0.3],\n",
    "    \"hovering\": [0.4, 0.4],\n",
    "    \"blocking\": [0.6, 0.2],\n",
    "    \"balanced\": [0.45, 0.35],\n",
    "}\n",
    "\n",
    "coop_qs = {\n",
    "    \"clockwise\": [0.8, 0.2],\n",
    "    \"pause_then_deliver\": [0.82, 0.18],\n",
    "    \"idle_helper\": [0.81, 0.19],\n",
    "    \"efficient\": [0.85, 0.15],\n",
    "}\n",
    "\n",
    "games = {\n",
    "    \"competitive\": competitive_qs,\n",
    "    \"mixed_motive\": mixed_qs,\n",
    "    \"cooperative\": coop_qs,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf3422d",
   "metadata": {},
   "source": [
    "### Interactive Temperature Parameter Exploration\n",
    "\n",
    "Explore how the temperature parameter affects the softness of best responses and strategic equivalence classes. As $\\tau \\to 0$, the soft best response approaches a hard best response (one-hot distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4378c764",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_br_distributions(tau):\n",
    "    fig = make_subplots(rows=1, cols=3, subplot_titles=[f\"{setting.title()} Setting\" for setting in games.keys()])\n",
    "    \n",
    "    for i, (setting, co_policies) in enumerate(games.items(), 1):\n",
    "        for policy_name, q_values in co_policies.items():\n",
    "            br = softmax(q_values, tau)\n",
    "            fig.add_trace(\n",
    "                go.Bar(name=policy_name, x=[\"Action 1\", \"Action 2\"], y=br),\n",
    "                row=1, col=i\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(height=400, width=1200, title=f\"Best Response Distributions (τ={tau:.2f})\")\n",
    "    fig.show()\n",
    "\n",
    "tau_slider = widgets.FloatSlider(value=0.1, min=0.01, max=2.0, step=0.01, description='Temperature (τ):')\n",
    "widgets.interactive(plot_br_distributions, tau=tau_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9170ebd",
   "metadata": {},
   "source": [
    "### Multi-timestep ΔSEC Tracking\n",
    "\n",
    "Track how strategic equivalence classes evolve over time as the agent interacts with different co-policies. The ΔSEC(t) metric captures the reduction in strategic ambiguity:\n",
    "\n",
    "$$\\Delta \\text{SEC}(t) = H(S_{t-1}) - H(S_t)$$\n",
    "\n",
    "where $H(S_t)$ is the entropy of the strategic neighborhood at time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7fbb2f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def simulate_interaction_trajectory(setting, n_steps=10):\n",
    "    co_policies = list(games[setting].keys())\n",
    "    trajectory = []\n",
    "    \n",
    "    # Start with a random policy\n",
    "    current_policy = np.random.choice(co_policies)\n",
    "    \n",
    "    for t in range(n_steps):\n",
    "        # Compute current SEC\n",
    "        current_br = softmax(games[setting][current_policy], tau=0.1)\n",
    "        sec = []\n",
    "        \n",
    "        for policy in co_policies:\n",
    "            br = softmax(games[setting][policy], tau=0.1)\n",
    "            if entropy(current_br, br) <= 0.05:\n",
    "                sec.append(policy)\n",
    "        \n",
    "        # Record SEC size and entropy\n",
    "        sec_size = len(sec)\n",
    "        sec_entropy = np.log(sec_size) if sec_size > 0 else 0\n",
    "        \n",
    "        trajectory.append({\n",
    "            'timestep': t,\n",
    "            'current_policy': current_policy,\n",
    "            'sec_size': sec_size,\n",
    "            'sec_entropy': sec_entropy\n",
    "        })\n",
    "        \n",
    "        # Transition to a new policy (simulating interaction)\n",
    "        current_policy = np.random.choice(co_policies)\n",
    "    \n",
    "    return pd.DataFrame(trajectory)\n",
    "\n",
    "def plot_trajectory(setting):\n",
    "    df = simulate_interaction_trajectory(setting)\n",
    "    \n",
    "    fig = make_subplots(rows=2, cols=1, subplot_titles=[\"SEC Size Over Time\", \"ΔSEC(t) Over Time\"])\n",
    "    \n",
    "    # Plot SEC size\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df['timestep'], y=df['sec_size'], mode='lines+markers', name='SEC Size'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Plot ΔSEC(t)\n",
    "    delta_sec = df['sec_entropy'].diff()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df['timestep'], y=delta_sec, mode='lines+markers', name='ΔSEC(t)'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=600, width=800, title=f\"{setting.title()} Setting: Strategic Equivalence Evolution\")\n",
    "    fig.show()\n",
    "\n",
    "setting_dropdown = widgets.Dropdown(options=list(games.keys()), description='Setting:')\n",
    "widgets.interactive(plot_trajectory, setting=setting_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e277e8c",
   "metadata": {},
   "source": [
    "### Strategic Influence Visualization\n",
    "\n",
    "Visualize how different co-policies influence the ego agent's best response distribution. The strategic influence matrix shows the KL divergence between best responses:\n",
    "\n",
    "$$I_{ij} = D_{KL}(BR^\\tau(\\pi_i) \\parallel BR^\\tau(\\pi_j))$$\n",
    "\n",
    "where $I_{ij}$ is the influence of policy $\\pi_i$ on policy $\\pi_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c714d4f1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_strategic_influence(base_policy, target_policy, setting):\n",
    "    base_br = softmax(games[setting][base_policy], tau=0.1)\n",
    "    target_br = softmax(games[setting][target_policy], tau=0.1)\n",
    "    return entropy(base_br, target_br)\n",
    "\n",
    "def plot_strategic_influence(setting):\n",
    "    co_policies = list(games[setting].keys())\n",
    "    influence_matrix = np.zeros((len(co_policies), len(co_policies)))\n",
    "    \n",
    "    for i, policy_i in enumerate(co_policies):\n",
    "        for j, policy_j in enumerate(co_policies):\n",
    "            influence_matrix[i, j] = compute_strategic_influence(policy_i, policy_j, setting)\n",
    "    \n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=influence_matrix,\n",
    "        x=co_policies,\n",
    "        y=co_policies,\n",
    "        colorscale='Viridis',\n",
    "        colorbar=dict(title='Strategic Influence')\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"{setting.title()} Setting: Strategic Influence Matrix\",\n",
    "        xaxis_title=\"Target Policy\",\n",
    "        yaxis_title=\"Base Policy\",\n",
    "        height=500,\n",
    "        width=600\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "setting_dropdown = widgets.Dropdown(options=list(games.keys()), description='Setting:')\n",
    "widgets.interactive(plot_strategic_influence, setting=setting_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cb6258",
   "metadata": {},
   "source": [
    "### Learning Progress Visualization\n",
    "\n",
    "Track the learning progress of the strategic embeddings over time. The loss function combines InfoNCE loss with a regularization term:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{InfoNCE}} + \\lambda \\|h(\\pi)\\|_2^2$$\n",
    "\n",
    "where $h(\\pi)$ is the embedding of policy $\\pi$ and $\\lambda$ is the regularization strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0b19cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embeddings(setting, n_epochs=100):\n",
    "    co_policies = list(games[setting].keys())\n",
    "    n_policies = len(co_policies)\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    state = create_train_state(rng, input_dim=2, embedding_dim=2, learning_rate=0.01)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i in range(n_policies):\n",
    "            # Get anchor, positive, and negative samples\n",
    "            anchor_policy = co_policies[i]\n",
    "            positive_policy = co_policies[(i + 1) % n_policies]\n",
    "            negative_policies = [co_policies[j] for j in range(n_policies) if j != i and j != (i + 1) % n_policies]\n",
    "            \n",
    "            # Compute best responses\n",
    "            anchor_br = softmax(games[setting][anchor_policy], tau=0.1)\n",
    "            positive_br = softmax(games[setting][positive_policy], tau=0.1)\n",
    "            negative_brs = [softmax(games[setting][p], tau=0.1) for p in negative_policies]\n",
    "            \n",
    "            # Create batch\n",
    "            batch = (anchor_br, positive_br, negative_brs)\n",
    "            \n",
    "            # Update model\n",
    "            state = train_step(state, batch)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = compute_infonce_loss(anchor_br, positive_br, negative_brs)\n",
    "            epoch_loss += loss\n",
    "        \n",
    "        losses.append(epoch_loss / n_policies)\n",
    "    \n",
    "    return state, losses\n",
    "\n",
    "def plot_learning_progress(setting):\n",
    "    state, losses = train_embeddings(setting)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(y=losses, mode='lines', name='Training Loss'))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"{setting.title()} Setting: Learning Progress\",\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=\"Loss\",\n",
    "        height=400,\n",
    "        width=800\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "setting_dropdown = widgets.Dropdown(options=list(games.keys()), description='Setting:')\n",
    "widgets.interactive(plot_learning_progress, setting=setting_dropdown) "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
